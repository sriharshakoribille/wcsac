{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c0277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implicit Quantile Network Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc521238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "184a0cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb95fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQN_SafetyCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim):\n",
    "        super(IQN_SafetyCritic, self).__init__()\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.K = 32  # number of samples for policy\n",
    "        self.N = 8  # number of quantile samples\n",
    "        self.n_cos = 64\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Start from 0 (according to paper)\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(self.n_cos)]).view(1,1,self.n_cos) \n",
    "\n",
    "        self.head = nn.Linear(obs_dim+action_dim, hidden_dim) \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, hidden_dim)\n",
    "        self.lin1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, 1)\n",
    "        #weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "    def calc_cos(self, batch_size, n_tau=8):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1) #(batch_size, n_tau, 1)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, obs, action, num_tau=8):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]  - actions size is 1 for us\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = obs.shape[0]\n",
    "        obs_action = torch.cat([obs, action], dim=-1)\n",
    "        \n",
    "        # cosine embedding\n",
    "        cos, taus = self.calc_cos(batch_size, num_tau)  # cos (bs, n_tau, n_cos), tau (bs, n_tau, 1)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos) # cos (bs*n_tau, n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.hidden_dim) # (bs, n_tau, hidden_dim)\n",
    "        # state-action embedding\n",
    "        x = torch.relu(self.head(obs_action))  # (bs, hidden_dim)\n",
    "        # combining embdeddings\n",
    "        # x has shape (batch, hidden_dim) for multiplication â€“> reshape to (batch, 1, hidden_dim)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.hidden_dim)\n",
    "        x = torch.relu(self.lin1(x))\n",
    "        out = self.lin2(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f8de9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 1])\n",
      "torch.Size([1, 8, 1])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "critic = IQN_SafetyCritic(obs_dim=10, action_dim=10, hidden_dim=64)\n",
    "s = torch.randn(1, 10)\n",
    "a = torch.randn(1, 10)\n",
    "distribution, quantiles = critic(s,a)\n",
    "print(distribution.shape)\n",
    "print(quantiles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4ec9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUANTILE REGRESSION LOSS + HUBER LOSS\n",
    "def quantile_huber_loss(td_errors, taus, k=1.0, n=8):\n",
    "        \"\"\"\n",
    "        Calculate quantiel huber loss element-wisely depending on kappa k and n (number of quantiles)\n",
    "        \"\"\"\n",
    "        # Huber Loss\n",
    "        huber_l = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "        assert huber_l.shape == (td_errors.shape[0], n, n), \"huber loss has wrong shape\"\n",
    "        # Quantile Huber Loss\n",
    "        quantil_l = abs(taus -(td_errors.detach() < 0).float()) * huber_l / 1.0\n",
    "        return quantil_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "541cd9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, ...\n",
    "batch_size = 12\n",
    "obs_dim = 10\n",
    "action_dim = 10\n",
    "num_quantiles = 8\n",
    "\n",
    "# initialize qr-networks\n",
    "critic = IQN_SafetyCritic(obs_dim=10, action_dim=10, hidden_dim=64)\n",
    "critic_target = IQN_SafetyCritic(obs_dim=10, action_dim=10, hidden_dim=64)\n",
    "\n",
    "# assume some batch_sample\n",
    "states = torch.randn(batch_size, obs_dim)\n",
    "actions = torch.randn(batch_size, action_dim)\n",
    "next_states = torch.randn(batch_size, obs_dim)\n",
    "rewards = torch.randn(batch_size, 1)\n",
    "dones = torch.zeros(batch_size, 1)\n",
    "# simulate policy\n",
    "next_action_new = torch.randn(batch_size, action_dim)\n",
    "\n",
    "# calculate q-targets\n",
    "# td-target: r + (gamma)*(1-d)*Q(next_state,a_new')\n",
    "q_target_next, _ = critic_target(next_states, next_action_new)  # (bs,N,1)\n",
    "q_target_next = q_target_next.transpose(1,2)  # (bs,1,N)\n",
    "assert q_target_next.shape == (batch_size,1, num_quantiles)\n",
    "q_target = rewards.unsqueeze(-1) + 0.99 * (1-dones.unsqueeze(-1)) * q_target_next\n",
    "q, taus = critic(states, actions)\n",
    "td = q_target - q\n",
    "assert td.shape == (batch_size, num_quantiles, num_quantiles)\n",
    "#print(q_target.shape)\n",
    "#print(q.shape)\n",
    "#print(td_error.shape)\n",
    "loss = quantile_huber_loss(td, taus)\n",
    "loss = loss.sum(dim=1).mean(dim=1)  # formula 8.5 // not entirely sure if we should sum first and then take mean\n",
    "loss = loss.mean()  # take mean over batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ceffc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1040, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262889d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
